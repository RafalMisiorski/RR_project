---
title: "Machine_Learning_2_Project_RM"
author: "Rafal Misiorski"
date: "10 02 2023"
output:
  html_document:
    df_print: paged
---


The mushroom dataset was used for classification problem.  
It consists 8124 observations with 23 variables describing a single mushroom. The target variable describes whether a mushroom is poisonous or edible.

```{r message=FALSE, warning=FALSE}
# Load required libraries
library(readr)
library(xgboost)
library(caret)
library(caret)
library(tidyinftheo)
library(mi)
library(infotheo)
Sys.setenv(LANG = "en")
data <- read_csv("mushrooms.csv")

colnames(data) <- gsub("-", "_", colnames(data))

# Check the new column names
colnames(data)
```

In the below part of code, there is a codndition added which states that is mushroom is edible, the class (target variable) will take value one, otherwise it will be zero.

```{r}
# Convert the 'classes' column to a binary target variable
data$class <- ifelse(data$class == "e", 1, 0)
# data$class <-  as.factor(data$class)
```

All variables are coverted to factors.
```{r}
# Convert other categorical variables to factors
data$cap_shape <- as.factor(data$cap_shape)
data$cap_surface <- as.factor(data$cap_surface)
data$cap_color <- as.factor(data$cap_color)
data$bruises <- as.factor(data$bruises)
data$odor <- as.factor(data$odor)
data$gill_attachment <- as.factor(data$gill_attachment)
data$gill_spacing <- as.factor(data$gill_spacing)
data$gill_size <- as.factor(data$gill_size)
data$gill_color <- as.factor(data$gill_color)
data$stalk_shape <- as.factor(data$stalk_shape)
data$stalk_root <- as.factor(data$stalk_root)
data$stalk_surface_above_ring <- as.factor(data$stalk_surface_above_ring)
data$stalk_surface_below_ring <- as.factor(data$stalk_surface_below_ring)
data$stalk_color_above_ring <- as.factor(data$stalk_color_above_ring)
data$stalk_color_below_ring <- as.factor(data$stalk_color_below_ring)
data$veil_type <- as.factor(data$veil_type)
data$veil_color <- as.factor(data$veil_color)
data$ring_number <- as.factor(data$ring_number)
data$ring_type <- as.factor(data$ring_type)
data$spore_print_color <- as.factor(data$spore_print_color)
data$population <- as.factor(data$population)
data$habitat <- as.factor(data$habitat)
data$habitat <- as.numeric(data$habitat)
```

```{r}
# Perform EDA
summary(data)
table(data$class)
```
3916 mushrooms were poisonous and 4208 were edible, therefore I can conclude that the dataset is quite balanced.

The only variable which is expandable is type of a veil (as value of factor "W" prevails other factors significantly).

The missing values are assigned as question marks in the dataset. Due to the fact that XGBoost and Random Forest are able to treat those missings as another category, they were not removed nor imputed.
```{r}
#Veil type had only one level, we can remove it then.
data <- subset(data, select = -c(veil_type) )
```

The dataset is split into training and test set with proportion 80/20.
```{r}
# Split the data into a train and test set
train_index <- createDataPartition(data$class, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

train_data[,-ncol(train_data)] <- sapply(train_data[,-ncol(train_data)], as.numeric)
test_data[,-ncol(test_data)] <- sapply(test_data[,-ncol(test_data)], as.numeric)

dtrain <- xgb.DMatrix(data = as.matrix(train_data[,-1]), label = train_data$class)
dtest <- xgb.DMatrix(data = as.matrix(test_data[,-1]))
```

The XGboost classifier model is used with 5 k fold cross validation. The chosen number of rounds is 100.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Train the model using the train data set
xgb_model <- xgb.train(data = dtrain, nrounds = 100, method = "xgbClassifier", trControl = trainControl(method = "cv", number = 5))

# Use the trained model to make predictions on the test data set
predictions <- predict(xgb_model, newdata = dtest)
```

The cut off point is set at 0.5 treshold.
```{r}
#setting the cut off point at 0.5
predictions <- ifelse(predictions > 0.5, 1, 0)
predictions <- as.factor(predictions)

test_data$class <- as.factor(test_data$class)
predictions <- as.factor(predictions)
levels(predictions) <- levels(test_data$class)
```

Evaluation of the model using test data set:
```{r}
# Evaluate the model's performance using the test data set
confusionMatrix(predictions, test_data$class)
```

The accuracy score on the test set is 100 percent. The dataset is balanced and were no missing values were present. Based on that the achieved score is feasible.

In the below section, the Random Forest classifier is used to predict class of a mushroom. The 5 k fold cross validation.

```{r message=FALSE, warning=FALSE}
#Random Forest

# Load the library
library(randomForest)

# Set the seed for reproducibility
set.seed(123)

# Define the training control
train_control <- trainControl(method = "cv", number = 5)

# Train the model
rf_model <- train(class ~ ., data = train_data, method = "rf", trControl = trainControl(method = "cv", number = 5))

#Predictions and test_y must be converted to factors with the same levels (0,1) to create a confusion matrix 
predictions <- predict(rf_model, newdata = test_data)
test_data$class <- as.numeric(test_data$class)
test_y <- test_data$class
predictions <- ifelse(predictions > 0.5, 1, 0)
test_y <- ifelse(test_y == 2, 1, 0)
test_y <- factor(test_y, levels = c("0","1"))
predictions <- factor(predictions, levels = c("0","1"))
```

The Plot of Confusion matrix:
```{r}
#Confusion matrix
confusionMatrix(predictions, test_y)
```

In case of the Random Forest, the 100 percent accuracy score is achieved as well. 

```{r, echo=FALSE}
#Importance plot
varImpPlot(rf_model$finalModel, main = "Random Forest Model")
```

Based on the Importance plot, The most important variable was the color of a gill, the second one was the color of a spore and the third one was the population. 

In conclusion, XGBoost and Random Forest algorithm have equally good performance. 




Regression Problem

The environment and console is cleared to perform regression algorithms.
```{r message=FALSE, warning=FALSE}
gc()
rm(list = ls())
```
The Energy dataset consists of energy use in Watt-hours in a house denoted every 10 minutes. The period of measurement lasted 4,5 months.

The main attributes are:
- The temperature in every room of a house
- Humidity in every room of a house
- Temperature outside
- Humidity outside
- Pressure
- Wind speed
- Visibility
- 2 random variables
- Time 


For the regression problem two algorithms were used: XGBoost and Convolutional Neural Networks.

The necessary libraries are loaded.
```{r message=FALSE, warning=FALSE, include=FALSE}
library(readr)
library(neuralnet)
library(xgboost)
library(caTools)
library(car)
library(quantmod)
library(MASS)
library(corrplot)
library(keras)
library(tensorflow)
library(psych)
library(lubridate)
library(dplyr)
library(caret)

Sys.setenv(LANG = "en")
```

First load the data
```{r message=FALSE, warning=FALSE}
data <- read_csv("energydata_complete.csv")
```

Exploratory data analysis
I'm Checking whether any missing values are present in the dataset
```{r}
print(sum(is.na(data)))
```
As per printed code, there are no missing values.

The dataset is summarized
```{r}
summary(data)
```

I am checking the data types of each column.
```{r}
sapply(data, class)
```
All variables except of the date are numeric. The only column which might be transformed is time as it might be broken down into diverse time intervals. Therefore, the year, month, day, hour, minute, and second variables are extracted.
```{r}
data$year <- year(data$date)
data$month <- month(data$date)
data$day <- day(data$date)
data$hour <- hour(data$date)
data$minute <- minute(data$date)
data$second <- second(data$date)
```

The new variable is created indicating the day of the week, hour of the day and time of the day (morning,afternoon or evening).

```{r}
data$day_of_week <- wday(data$date, label = TRUE)
data$hour_of_day <- hour(data$date)
data$time_of_day <- ifelse(data$hour_of_day >= 6 & data$hour_of_day < 12, "morning",
                           ifelse(data$hour_of_day >= 12 & data$hour_of_day < 18, "afternoon", "evening"))
```

All variables are converted to numeric data type.

```{r}
numeric_columns <- which(sapply(data, is.numeric))
data_numeric <- data[, numeric_columns]
```

The date variable is removed from the dataset as other time related variables were added recently. The Random Variable 1 and 2 are removed as the author of the dataset added it on purpose to make additional noise of data. 

```{r}
data_numeric <- dplyr::select(data_numeric, -second, -year, -hour_of_day, -rv1, -rv2)
```

The regression model was created with all variables to check if the multicollinearity is present.

```{r}
model_all <- lm(Appliances ~ ., data=data_numeric) 
summary(model_all)
vif(model_all)
```

As per shown Variance Inflated factor, there are a lot of variables which are correlated to each other (the VIF exceeds 4 treshold). Therefore I remove iteratively variables with high VIF and check again if the model consists highly correlated variables.

```{r message=TRUE, warning=TRUE, include=FALSE}
#I will remove TDewpoint,
data_numeric <- dplyr::select(data_numeric, -Tdewpoint)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

#T9 removed
data_numeric <- dplyr::select(data_numeric, -T9)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

#Month removed
data_numeric <- dplyr::select(data_numeric, -month)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

#T6 removed
data_numeric <- dplyr::select(data_numeric, -T6)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

#T2 removed
data_numeric <- dplyr::select(data_numeric, -T2)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

#T7 removed
data_numeric <- dplyr::select(data_numeric, -T7)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

#RH_4 removed
data_numeric <- dplyr::select(data_numeric, -RH_4)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# #RH_1 removed
data_numeric <- dplyr::select(data_numeric, -RH_1)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# T1 removed
data_numeric <- dplyr::select(data_numeric, -T1)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# RH_7 removed
data_numeric <- dplyr::select(data_numeric, -RH_7)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)


# RH_3 removed
data_numeric <- dplyr::select(data_numeric, -RH_3)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# RH_8 removed
data_numeric <- dplyr::select(data_numeric, -RH_8)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# T5 removed
data_numeric <- dplyr::select(data_numeric, -T5)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# RH_6 removed
data_numeric <- dplyr::select(data_numeric, -RH_6)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# T3 removed
data_numeric <- dplyr::select(data_numeric, -T3)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)

# RH_2 removed
data_numeric <- dplyr::select(data_numeric, -RH_2)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)
```

The minute variable is almost uncorrelated with other variables, hence the it will be removed from the dataset as well.

```{r include=FALSE}
# minute removed
data_numeric <- dplyr::select(data_numeric, -minute)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)
```

```{r echo=TRUE}
# T4 removed
data_numeric <- dplyr::select(data_numeric, -T4)
model_all <- lm(Appliances ~ ., data=data_numeric)
vif(model_all)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
corPlot(data_numeric, cex = 0.5, margin = c(4,4,4,4))
```
The above plot shows that the variables are correlated to the moderate level at most.

It turns out that there is no need to keep date, day of week and time of day variables.
```{r}
data <- dplyr::select(data, -date)
data <- dplyr::select(data, -day_of_week)
data <- dplyr::select(data, -time_of_day)

# Create a vector of predictors
predictors <- names(data_numeric)
predictors <- predictors[predictors != "Appliances"]
```

The independent variables are standardized to avoid biased results of predictions.

```{r}
# Subset the data to include only the predictors
subset_data <- data_numeric[, predictors]

# Standardize the predictors
subset_data <- scale(subset_data)

# Re-combine the standardized predictors with the "Appliances" variable
data_numeric[, predictors] <- subset_data
```

The dataset is divided into training and test set with ratio 80/20.
```{r}
set.seed(123)
data <- data_numeric
# split the data into training and testing data
train_indices <- sample(1:nrow(data), 0.8*nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define the predictor variables
predictors <- names(data)[-c(1,2)]
response <- "Appliances"
```

Both sets are converted into DMatrix
```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_data[predictors]), label = unlist(train_data[response]))
dtest <- xgb.DMatrix(data = as.matrix(test_data[predictors]))
```

The XGBoost model is trained. After many trials with hyperparameter tuning, the following parameters were chosen:
lambda = 0.8
alpha =0.9 
nrounds = 100
colsample_bytree = 0.6 
gamma=0.85 
eta = 0.036 
max_depth = 25, 
min_child_weight = 1, 
subsample=0.8, 
num_parallel_tree = 4 
early_stopping_rounds = 50

The most significant hyperparameters was eta and number of parallel trees.

```{r include=FALSE}
# Train the XGBoost model on the training data
xgb_model <- xgboost(data = dtrain,objective = "reg:squarederror",lambda = 0.8,alpha =0.9, nrounds = 100,colsample_bytree = 0.6, gamma=0.85 , eta = 0.036, max_depth = 25, min_child_weight = 1, subsample=0.8, num_parallel_tree = 4, early_stopping_rounds = 50)
```

The predictions on test data and MAPE are generated.
```{r}

# Make predictions on the test data
predictions <- predict(xgb_model, newdata = dtest)

# Extract the response variable from the test data as a vector
test_response <- unlist(test_data[response])

# Calculate the absolute percentage error
ape <- abs((test_response - predictions) / test_response)

# Calculate the mean absolute percentage error
MAPE <- mean(ape) * 100

# Print the result
print(MAPE)
```

The Result is MAPE: 24 percent.

The next used algorithm is Convolutional Neural Network trained in Keras library. The used division of training and test set is 70/30. The model has 3 hidden layers with 16, 4 and 2 units. Such setup allows to results with closed MAPE on training and validation set.  The loss function is mean squared error with Rectified linear unit activation function with 100 epochs. To compare results with the XGBoost algorithm, the mean absolut percentage error is also used as an evalutional metric. Due to the fact that the validation MAPE was lower than MAPE from the training set, I used L2 Regularization to reduce overfitting effect. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Preprocess the data
data_train <- data[1:round(0.7*nrow(data)),]
data_test <- data[(round(0.7*nrow(data))+1):nrow(data),]
x_train <- as.matrix(data_train[,-c(1,2)])
y_train <- as.matrix(data_train[,2])
x_test <- as.matrix(data_test[,-c(1,2)])
y_test <- as.matrix(data_test[,2])

# Build the model with L2 regularization
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = ncol(x_train),
              kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 2, activation = "relu", kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 1, activation = "relu", kernel_regularizer = regularizer_l2(0.01)) 

# Define a function to calculate MAPE
mape <- function(y_true, y_pred) {
  return(mean(abs((y_true - y_pred) / y_true)) * 100)
}

# Compile the model with MAPE as a metric
model %>% compile(loss = "mean_squared_error", optimizer = "adam", 
                  metrics = c("mean_absolute_percentage_error", mape))

# Fit the model and evaluate its performance
history <- model %>% fit(x_train, y_train, epochs = 100, batch_size = 32, 
                         validation_data = list(x_test, y_test))

```

```{r echo=FALSE}
# Plot the MAPE values during training
plot(history)
```
The above plot presents that the MAPE on the training set is equal to 5.4 percent after 100 epochs. On the validation set the MAPE is equal to 4.74 percent. Due to the fact that the MAPE on the validation set is slightly lower than MAPE from the training set, the overfitting effect might be still present. The results are consistent for training set all epochs. The points validation restate a line which is almost parallel to the training one. 

The CNN outperformed the XGBoost algorithm (4.74 percent vs 25 percent). Unfortunately the result might be biased. The complexity of the CNN model might was reduced as possible, therefore more training data could improved the results.

Sources:
Appliances energy prediction Data Set:
https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction

Mushroom classification https://archive.ics.uci.edu/ml/datasets/Mushroom
https://www.kaggle.com/datasets/uciml/mushroom-classification

